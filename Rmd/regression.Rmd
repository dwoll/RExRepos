---
license: Creative Commons BY-SA
author: Daniel Wollschlaeger
title: "Multiple linear regression"
categories: [Univariate, Regression]
rerCat: Univariate
tags: [Regression]
---

Multiple linear regression
=========================

```{r echo=FALSE}
knit_hooks$set(rgl=hook_rgl)
```

TODO
-------------------------

 - link to regressionDiag, regressionModMed, crossvalidation, resamplingBootALM

Install required packages
-------------------------

[`car`](http://cran.r-project.org/package=car), [`leaps`](http://cran.r-project.org/package=leaps), [`lmtest`](http://cran.r-project.org/package=lmtest), [`QuantPsyc`](http://cran.r-project.org/package=QuantPsyc), [`robustbase`](http://cran.r-project.org/package=robustbase), [`sandwich`](http://cran.r-project.org/package=sandwich)

```{r}
wants <- c("car", "leaps", "lmtest", "QuantPsyc", "robustbase", "sandwich")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
```

Descriptive model fit
-------------------------

### Descriptive model fit

```{r}
set.seed(123)
N  <- 100
X1 <- rnorm(N, 175, 7)
X2 <- rnorm(N,  30, 8)
X3 <- abs(rnorm(N, 60, 30))
Y  <- 0.5*X1 - 0.3*X2 - 0.4*X3 + 10 + rnorm(N, 0, 7)
dfRegr <- data.frame(X1, X2, X3, Y)
```

```{r}
(fit12 <- lm(Y ~ X1 + X2, data=dfRegr))
lm(scale(Y) ~ scale(X1) + scale(X2), data=dfRegr)
```

```{r rerRegression01, rgl=TRUE}
library(car)
scatter3d(Y ~ X1 + X2, fill=FALSE, data=dfRegr)
```

### Estimated coefficients, residuals, and fitted values

```{r}
coef(fit12)
E <- residuals(fit12)
head(E)
Yhat <- fitted(fit12)
head(Yhat)
```

### Add and remove predictors

```{r}
(fit123 <- update(fit12,  . ~ . + X3))
(fit13 <- update(fit123, . ~ . - X1))
(fit1 <- update(fit12,  . ~ . - X2))
```

Assessing model fit
-------------------------

### (Adjusted) \(R^{2}\) and residual standard error

```{r}
sumRes <- summary(fit123)
sumRes$r.squared
sumRes$adj.r.squared
sumRes$sigma
```

### Information criteria AIC and BIC

```{r}
AIC(fit1)
extractAIC(fit1)
extractAIC(fit1, k=log(N))
```

### Crossvalidation

`cv.glm()` function from package `boot`, see crossvalidation

Coefficient tests and overall model test
-------------------------

```{r}
summary(fit12)
confint(fit12)
vcov(fit12)
```

Variable selection and model comparisons
-------------------------

### Model comparisons

#### Effect of adding a single predictor

```{r}
add1(fit1, . ~ . + X2 + X3, test="F")
```

#### Effect of adding several predictors

```{r}
anova(fit1, fit123)
```

### All predictor subsets

```{r rerRegression02}
data(longley)
head(longley)
library(leaps)
subs <- regsubsets(GNP.deflator ~ ., data=longley)
summary(subs, matrix.logical=TRUE)
plot(subs, scale="bic")
```

```{r}
Xmat <- data.matrix(subset(longley, select=c("GNP", "Unemployed", "Population", "Year")))
(leapFits <- leaps(Xmat, longley$GNP.deflator, method="Cp"))
```

```{r rerRegression03}
plot(leapFits$size, leapFits$Cp, xlab="model size", pch=20, col="blue",
     ylab="Mallows' Cp", main="Mallows' Cp agains model size")
abline(a=0, b=1)
```

Apply regression model to new data
-------------------------

```{r}
X1new <- c(177, 150, 192, 189, 181)
dfNew <- data.frame(X1=X1new)
predict(fit1, dfNew, interval="prediction", level=0.95)
predOrg <- predict(fit1, interval="confidence", level=0.95)
```

```{r rerRegression04}
hOrd <- order(X1)
par(lend=2)
plot(X1, Y, pch=20, xlab="Predictor", ylab="Dependent variable and prediction",
     xaxs="i", main="Data and regression prediction")

polygon(c(X1[hOrd],             X1[rev(hOrd)]),
        c(predOrg[hOrd, "lwr"], predOrg[rev(hOrd), "upr"]),
        border=NA, col=rgb(0.7, 0.7, 0.7, 0.6))

abline(fit1, col="black")
legend(x="bottomright", legend=c("Data", "prediction", "confidence region"),
       pch=c(20, NA, NA), lty=c(NA, 1, 1), lwd=c(NA, 1, 8),
       col=c("black", "blue", "gray"))
```

Robust and penalized regression
-------------------------

### Robust regression

Heteroscedasticity-consistent standard errors (modified White estimator):
`hccm()` from package `car` or `vcovHC()` from package `sandwich`.
These standard errors can then be used in combination with function `coeftest()` from package `lmtest()`.

```{r}
library(car)
library(lmtest)
fitLL <- lm(GNP.deflator ~ ., data=longley)
summary(fitLL)
coeftest(fitLL, vcov=hccm)
```

```{r results='hide'}
library(sandwich)
coeftest(fitLL, vcov=vcovHC)
```

 - \(M\)-estimators: `rlm()` from package `MASS`
 - resistant regression: `lqs()` from package `MASS`

More information can be found in CRAN task view [Robust Statistical Methods](http://cran.r-project.org/web/views/Robust.html).
 
### Penalized regression

#### Ridge regression

```{r}
library(car)
vif(fitLL)
```

```{r}
library(MASS)
lambdas <- 10^(seq(-8, -1, length.out=200))
lmrFit  <- lm.ridge(GNP.deflator ~ ., lambda=lambdas, data=longley)
select(lmrFit)
```

```{r rerRegression05}
lmrCoef <- coef(lmrFit)
plot(lmrFit, xlab="lambda", ylab="coefficients")
plot(lmrFit$lambda, lmrFit$GCV, type="l", xlab="lambda", ylab="GCV")
```

See packages [`lars`](http://cran.r-project.org/package=lars) and [`glmnet`](http://cran.r-project.org/package=glmnet) for the LASSO and elastic net methods which combine regularization and selection.

Detach (automatically) loaded packages (if possible)
-------------------------

```{r}
try(detach(package:QuantPsyc))
try(detach(package:leaps))
try(detach(package:lmtest))
try(detach(package:sandwich))
try(detach(package:zoo))
try(detach(package:car))
try(detach(package:nnet))
try(detach(package:MASS))
```
